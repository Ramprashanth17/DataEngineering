{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37cb1aca-5b09-4fab-9fe4-ccf75c694f2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Delta Live Tables (DLT)\n",
    "\n",
    "- DLT is a declarative ETL framework for building reliable, maintainable, and testable data processing pipelines.\n",
    "- You define transformation to perform on the data, and DLT manages task orchestration, cluster management, monitoring, data quality, and error handling.\n",
    "- It handles both streaming and batch workloads with minimal manual intervention.\n",
    "\n",
    "Declarative ETL: Use either SQL or Python to define what needs to be done and DLT is going to take care of the ETL part.\n",
    "DLT abstracts stream and batch workloads. \n",
    "\n",
    "Without DLT, in the medallion architecture fw for the Data Lake, due to relations between notebooks and data dependencies are kinda cumbersome. Data quality checks needs to be done separately, dependency management or task orchestration using 3rd party software, manual checkpoint and re-tries, incremental processing and Infrastructure management. Data Lineage issues are persistent, and all these problems are complex and time-consuming.\n",
    "\n",
    "DLT helps in solving these problems, you can write Data quality checks, automated dependency management, auto checkpoint and re-tries, incremental processing, it simplifies infrastructure mgmt, you can also see the Data Lineage in its UI\n",
    "\n",
    "Databricks Jobs:It manages the workflow, scheduling tasks (including DLt), trigger tasks based on events; orchestrate ETL, BI, ML tasks\n",
    "\n",
    "DLT pipelines: Performs task orchestration within a pipeline for managing a data flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abd1ba00-9805-4265-b71c-5f5832b65074",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### DLT Architecture\n",
    "\n",
    "Built upon Apache Spark and Delta Lake, you can do either batch or streaming workloads and you can config workloads to both continuous working mode or trigger mode. \n",
    "UI --> Notebooks (DLT/Spark notebooks) [SQL\\Python not Scala], it contains data transformation and DQuality expectations.\n",
    "DLT pipelines ---> It first gathers all the tasks present in the notebooks and does a DAG. It provisions the cluster ---> Job or serverless clusters; all purpose is not supported.\n",
    "\n",
    "Gathers metadata --> Pipeline status, DQ checks and Data Lineage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fca55234-75e9-4092-8a32-7d4c8dca3e18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Programming using DLT\n",
    "\n",
    "Imperative is where you specify the task in greater detail to get the result. Ex: C or Java program\n",
    "Declarative programming is where you just specify what you want and the compiler does the rest. Ex: SQL query.\n",
    "\n",
    "Advantages of Declarative Program:\n",
    "1. No need to handle checkpoints, fault tolerance, dependencies, etc.\n",
    "2. No need to write complex logic such as CDC, Schema Evolution, etc.\n",
    "3. Better Optimization\n",
    "\n",
    "In the bronze layer, you'll have multiple sources of data and the first step of transformation here is in Notebooks where you'll specify the instructions to read source, transform the data, check the quality and finally create live datasets which are stored in the Delta Live Tables or in Delta Lake.These datasets can be accessible only by DLT pipeline, to make it accessible outside of DLT pipleine you need to publish the dataset to unity catalog. The live datasets are of two types: 1. Streaming Tables, 2. Materialized Views and Views\n",
    "\n",
    "Streaming Tables: Delta Table to which streams write data, reads from eventhub, kafka, cloud files, delta tables, offers exactly once guarantee. Incremenal data ingestion workloads, low latency transformations, massive amount of data, DML ops allowed.\n",
    "\n",
    "Materialized Views: Delta Table created from the result of a query. Full refresh data ingestion workloads,build aggregate tables for reporting, improve latency of BI reports, data transformation workloads, no DML ops allowed, change the query instead.\n",
    "\n",
    "Views: No physical storage of the data, scope limited to the pipeline, can't be published to Unity Catalog, stores intermediate results to reduce complexity to enforce data quality constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97f3df2d-25be-4302-a536-5a3f5c46494f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "/*\n",
    "create or refresh live dataset [dq expectations]\n",
    "as\n",
    "select col, transformations from source\n",
    "*/ \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Delta_Live_Tables",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
