{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQDIsbnalTSJ1XCQ/yBNYT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ramprashanth17/DataEngineering/blob/main/Databricks_DE/Practice/AdvOps_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grouping Data and Types of Joins in Spark\n",
        "\n",
        "We can group data based on different cols in a DataFrame, and apply a different aggregations such as sum or avg to get a holistic view of data slices.\n",
        "\n",
        "```\n",
        "salary_data.groupby('Department')\n",
        "```\n",
        "\n",
        "This returns a grouped data object and can be assigned to a separate DF and more ops can be done on this data.\n",
        "\n",
        "Ex:\n",
        "```\n",
        "salary_data.groupby('Department').avg().show()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "egibBlJqmL9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A complex groupBy Statement\n",
        "\n",
        "groupBy can be used in complex data ops, such as multiple aggregations within a single groupBy statement.\n",
        "\n"
      ],
      "metadata": {
        "id": "Xz1oUqG2noAe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8y12SqBcmH4B"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, round\n",
        "salary_data.groupBy('Department')\\\n",
        "  .sum('Salary')\\\n",
        "  .withColumn('sum(Salary)', round(col('sum(Salary)'), 2)) \\\n",
        "  .withColumnRenamed('sum(Salary)', 'Salary') \\\n",
        "  .orderBy('Department')\\\n",
        "  .show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Joining DataFrames in Spark\n",
        "\n",
        "Join ops are used to combine data from two or more DataFrames based on a common column. It is essentially for merging datasets. aggregations, and relational operations.\n",
        "\n",
        "**.join()** is the method to join and its parameters are:\n",
        "\n",
        "- **other** The other dataframe to join with.\n",
        "- **on**: Cols on which to join the DataFrames\n",
        "- **how**: Type of join\n",
        "- **suffixes**: Suffixes to add to cols with the same name in both dataframes\n",
        "\n",
        "```\n",
        "DF1.join(DF2, on, how)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "cxnaTOuWo79P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "salary_data_with_id = [(1, \"John\", \"Field-eng\", 3500), \\\n",
        "    (2, \"Robert\", \"Sales\", 4000), \\\n",
        "    (3, \"Maria\", \"Finance\", 3500), \\\n",
        "    (4, \"Michael\", \"Sales\", 3000), \\\n",
        "    (5, \"Kelly\", \"Finance\", 3500), \\\n",
        "    (6, \"Kate\", \"Finance\", 3000), \\\n",
        "    (7, \"Martin\", \"Finance\", 3500), \\\n",
        "    (8, \"Kiran\", \"Sales\", 2200), \\\n",
        "  ]\n",
        "\n",
        "\n",
        "columns= [\"ID\", \"Employee\", \"Department\", \"Salary\"]\n",
        "salary_data_with_id = spark.createDataFrame(data = salary_data_with_id, schema = columns)\n",
        "salary_data_with_id.show()"
      ],
      "metadata": {
        "id": "T4U7RbiGqDMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "employee_data = [(1, \"NY\", \"M\"), \\\n",
        "    (2, \"NC\", \"M\"), \\\n",
        "    (3, \"NY\", \"F\"), \\\n",
        "    (4, \"TX\", \"M\"), \\\n",
        "    (5, \"NY\", \"F\"), \\\n",
        "    (6, \"AZ\", \"F\") \\\n",
        "  ]\n",
        "columns= [\"ID\", \"State\", \"Gender\"]\n",
        "employee_data = spark.createDataFrame(data = employee_data, schema = columns)\n",
        "employee_data.show()"
      ],
      "metadata": {
        "id": "Kmp_OdruqSoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dZmuPyTLqnm5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}