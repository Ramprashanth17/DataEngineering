{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTub+2SpWkmyAo3cS3pHjN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ramprashanth17/DataEngineering/blob/main/Databricks_DE/Practice/AdvOps_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grouping Data and Types of Joins in Spark\n",
        "\n",
        "We can group data based on different cols in a DataFrame, and apply a different aggregations such as sum or avg to get a holistic view of data slices.\n",
        "\n",
        "```\n",
        "salary_data.groupby('Department')\n",
        "```\n",
        "\n",
        "This returns a grouped data object and can be assigned to a separate DF and more ops can be done on this data.\n",
        "\n",
        "Ex:\n",
        "```\n",
        "salary_data.groupby('Department').avg().show()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "egibBlJqmL9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A complex groupBy Statement\n",
        "\n",
        "groupBy can be used in complex data ops, such as multiple aggregations within a single groupBy statement.\n",
        "\n"
      ],
      "metadata": {
        "id": "Xz1oUqG2noAe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8y12SqBcmH4B"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, round\n",
        "salary_data.groupBy('Department')\\\n",
        "  .sum('Salary')\\\n",
        "  .withColumn('sum(Salary)', round(col('sum(Salary)'), 2)) \\\n",
        "  .withColumnRenamed('sum(Salary)', 'Salary') \\\n",
        "  .orderBy('Department')\\\n",
        "  .show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Joining DataFrames in Spark\n",
        "\n",
        "Join ops are used to combine data from two or more DataFrames based on a common column. It is essentially for merging datasets. aggregations, and relational operations.\n",
        "\n",
        "**.join()** is the method to join and its parameters are:\n",
        "\n",
        "- **other** The other dataframe to join with.\n",
        "- **on**: Cols on which to join the DataFrames\n",
        "- **how**: Type of join\n",
        "- **suffixes**: Suffixes to add to cols with the same name in both dataframes\n",
        "\n",
        "```\n",
        "DF1.join(DF2, on, how)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "cxnaTOuWo79P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "salary_data_with_id = [(1, \"John\", \"Field-eng\", 3500), \\\n",
        "    (2, \"Robert\", \"Sales\", 4000), \\\n",
        "    (3, \"Maria\", \"Finance\", 3500), \\\n",
        "    (4, \"Michael\", \"Sales\", 3000), \\\n",
        "    (5, \"Kelly\", \"Finance\", 3500), \\\n",
        "    (6, \"Kate\", \"Finance\", 3000), \\\n",
        "    (7, \"Martin\", \"Finance\", 3500), \\\n",
        "    (8, \"Kiran\", \"Sales\", 2200), \\\n",
        "  ]\n",
        "\n",
        "\n",
        "columns= [\"ID\", \"Employee\", \"Department\", \"Salary\"]\n",
        "salary_data_with_id = spark.createDataFrame(data = salary_data_with_id, schema = columns)\n",
        "salary_data_with_id.show()"
      ],
      "metadata": {
        "id": "T4U7RbiGqDMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "employee_data = [(1, \"NY\", \"M\"), \\\n",
        "    (2, \"NC\", \"M\"), \\\n",
        "    (3, \"NY\", \"F\"), \\\n",
        "    (4, \"TX\", \"M\"), \\\n",
        "    (5, \"NY\", \"F\"), \\\n",
        "    (6, \"AZ\", \"F\") \\\n",
        "  ]\n",
        "columns= [\"ID\", \"State\", \"Gender\"]\n",
        "employee_data = spark.createDataFrame(data = employee_data, schema = columns)\n",
        "employee_data.show()"
      ],
      "metadata": {
        "id": "Kmp_OdruqSoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **INNER JOIN**\n",
        "\n",
        "Used to join two DF's based on values that are common in both DF's. Any value that doesn't exist in any one of the DFs wouldn't be part of the resulting DF.\n",
        "\n",
        "***Default*** type of join in Spark.\n",
        "\n",
        "Use Case:\n",
        "\n",
        "Inner joins are useful for merging data when you are interested in common elements in both DataFrames – for example, joining sales data with customer data to see which customers made a purchase."
      ],
      "metadata": {
        "id": "7PNHxlQfrBgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "salary_data_with_id.join(employee_data,salary_data_with_id.ID == employee_data.ID, \"inner\").show()"
      ],
      "metadata": {
        "id": "dZmuPyTLqnm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **OUTER JOIN**\n",
        "\n",
        "AKA ***full outer join*** returns all rows from both DF's, filling missing values with ***null***.\n",
        "\n",
        "We should use an outer join when we want to join two DataFrames based on values that exist in both DataFrames, regardless of whether they don’t exist in the other DataFrame. Any values that exist in any one of the DataFrames would be part of the resulting DataFrame.\n",
        "\n",
        "**Use Case**:\n",
        "Use it when you want to include all records from both DataFrames while accommodating unmatched values- for ex, merging employee data with project data to see which employees are assigned to which projects, including those who are unassigned."
      ],
      "metadata": {
        "id": "G4wUZxLrr4a9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "salary_data_with_id.join(employee_data, salary_data_with_id.ID == employee_data.ID, \"outer\").show()"
      ],
      "metadata": {
        "id": "w9ThektPspxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **LEFT JOIN**\n",
        "\n",
        "Left Join returns all the rows from the left DF and the matched rows from the right DF. If there is no match in the right DF, the result will contain **null** values.\n",
        "\n",
        "**Use Case**\n",
        "\n",
        "Useful when you want to keep all the records from the left DF and only the matching records from the right DF. Ex: Merging customer data with transaction data to see which customers have made a purchase.\n",
        "\n"
      ],
      "metadata": {
        "id": "tdus3cF1tFRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "salary_data_with_id.join(employee_data, salary_data_with_id.ID == employee_data.ID, \"left\").show()"
      ],
      "metadata": {
        "id": "FgGKUM-bt0t0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **RIGHT JOIN**\n",
        "\n",
        "Similar to Left Join, but returns all the rows from the right DF and the matched rows from the left DF. Non matching rows from the left DF contain null values."
      ],
      "metadata": {
        "id": "B-llEgy1uIIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "salary_data_with_id.join(employee_data, salary_data_with_id.ID == employee_data.ID, \"right\").show()\n"
      ],
      "metadata": {
        "id": "xa0lD8Hhuasq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **CROSS JOIN**\n",
        "\n",
        "AKA Cartesian Join, combines each row from the left DF with every row from the right DF.\n",
        "\n",
        "Typically used when you want to explore all possible combinations of data, such as when generating test data.\n"
      ],
      "metadata": {
        "id": "miQPYsnoulWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **UNION**\n",
        "\n",
        "Union is used to join 2 DFs having similar schema."
      ],
      "metadata": {
        "id": "GpmOP_ejvFqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "salary_data_with_id_2 = [(1, \"John\", \"Field-eng\", 3500), \\\n",
        "    (2, \"Robert\", \"Sales\", 4000), \\\n",
        "    (3, \"Aliya\", \"Finance\", 3500), \\\n",
        "    (4, \"Nate\", \"Sales\", 3000), \\\n",
        "  ]\n",
        "columns2= [\"ID\", \"Employee\", \"Department\", \"Salary\"]\n",
        "salary_data_with_id_2 = spark.createDataFrame(data = salary_data_with_id_2, schema = columns2)\n",
        "salary_data_with_id_2.printSchema()\n",
        "salary_data_with_id_2.show(truncate=False)"
      ],
      "metadata": {
        "id": "-8gAPYu5vabm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unionDF = salary_data_with_id.union(salary_data_with_id_2)\n",
        "unionDF.show(truncate=False)"
      ],
      "metadata": {
        "id": "BVr_p5CfvbQu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}