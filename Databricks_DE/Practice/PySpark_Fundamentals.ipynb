{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKXOs2+6COMaI7XEf8iZv7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ramprashanth17/DataEngineering/blob/main/Databricks_DE/Practice/PySpark_Fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark Dataframes and their Operations\n"
      ],
      "metadata": {
        "id": "LALnu_FPdsbj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99WitwaUdlNh",
        "outputId": "1a3b4cf2-ff36-435b-a5c4-ced6f6354986"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n"
          ]
        }
      ],
      "source": [
        "## Installing PySpark\n",
        "\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once Spark has been installed, we have to create Spark session which acts as our entry point for any Spark Application."
      ],
      "metadata": {
        "id": "RiRX_A4QeM58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Creating a spark session\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "NAuVodfIeEtY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If using spark shell, we need not run the above code as the session is auto created. Note that, only one Spark session can be created at any given time and duplicating a Spark session is not possible!"
      ],
      "metadata": {
        "id": "iSEyPJWcerA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset or DataFrame API\n",
        "\n",
        "The Dataset API is a distributed collection of data, it is available in Java and Scala not in Python and R. The API uses RDDs (Resilient Distributed Datasets) it provides fixed typing.\n",
        "\n",
        "For Python and R users, we have the similar Dataframe API, influenced by Pandas Dataframes in Python. It is essentially like a table, with the table headers as column names and below these headers are data arranged accordingly. This API was also built on top of RDDs, and Dataframes abstracts from the complexity of RDD, Dataframes are also lazily evaluated and are immutable."
      ],
      "metadata": {
        "id": "OXWJmoktfT22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By this lazy evaluation, Spark has performance gains and optimization by running the computations only when needed. Computations start only when an action is called on a DataFrame."
      ],
      "metadata": {
        "id": "ERSeyLbngmaR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v8NcFCITeekk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}