{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFDPCz90nGf7UCQfN+fFoY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ramprashanth17/DataEngineering/blob/main/Databricks_DE/Practice/PySpark_Fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark Dataframes and their Operations\n"
      ],
      "metadata": {
        "id": "LALnu_FPdsbj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99WitwaUdlNh",
        "outputId": "1a3b4cf2-ff36-435b-a5c4-ced6f6354986"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n"
          ]
        }
      ],
      "source": [
        "## Installing PySpark\n",
        "\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once Spark has been installed, we have to create Spark session which acts as our entry point for any Spark Application."
      ],
      "metadata": {
        "id": "RiRX_A4QeM58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Creating a spark session\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "NAuVodfIeEtY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If using spark shell, we need not run the above code as the session is auto created. Note that, only one Spark session can be created at any given time and duplicating a Spark session is not possible!"
      ],
      "metadata": {
        "id": "iSEyPJWcerA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset or DataFrame API\n",
        "\n",
        "The Dataset API is a distributed collection of data, it is available in Java and Scala not in Python and R. The API uses RDDs (Resilient Distributed Datasets) it provides fixed typing.\n",
        "\n",
        "For Python and R users, we have the similar Dataframe API, influenced by Pandas Dataframes in Python. It is essentially like a table, with the table headers as column names and below these headers are data arranged accordingly. This API was also built on top of RDDs, and Dataframes abstracts from the complexity of RDD, Dataframes are also lazily evaluated and are immutable."
      ],
      "metadata": {
        "id": "OXWJmoktfT22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By this lazy evaluation, Spark has performance gains and optimization by running the computations only when needed. Computations start only when an action is called on a DataFrame."
      ],
      "metadata": {
        "id": "ERSeyLbngmaR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating DataFrame Operations\n",
        "\n",
        "DataFrames are the main building blocks of Spark data, they consist of rows and column data structures.\n",
        "\n",
        "You can specify the schema of the dataframe either explicitly or let it infer from the Dataframe directly."
      ],
      "metadata": {
        "id": "tufQfnL8hJqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Creating DataFrame using a list of rows\n",
        "\n",
        "```\n",
        "import pandas as pd\n",
        "from datetime import datetime, date\n",
        "from pyspark.sql import Row\n",
        "data_df = spark.createDataFrame([\n",
        "    Row(col_1=100, col_2=200., col_3='string_test_1', col_4=date(2023, 1, 1), col_5=datetime(2023, 1, 1, 12, 0)),\n",
        "    Row(col_1=200, col_2=300., col_3='string_test_2', col_4=date(2023, 2, 1), col_5=datetime(2023, 1, 2, 12, 0)),\n",
        "    Row(col_1=400, col_2=500., col_3='string_test_3', col_4=date(2023, 3, 1), col_5=datetime(2023, 1, 3, 12, 0))\n",
        "]\n",
        "\n",
        "# To define schema explicitly\n",
        ", schema=' col_1 long, col_2 double, col_3 string, col_4 date, col_5 timestamp')\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "0b_runFNh-Bm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Creating Pandas Dataframes\n",
        "First create a DataFrame using Pandas first, later convert that dataframe to PySpark dataframe.\n",
        "\n",
        "```\n",
        "from datetime import datetime, date\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "rdd = spark.sparkContext.parallelize([\n",
        "    (100, 200., 'string_test_1', date(2023, 1, 1), datetime(2023, 1, 1, 12, 0)),\n",
        "    (200, 300., 'string_test_2', date(2023, 2, 1), datetime(2023, 1, 2, 12, 0)),\n",
        "    (300, 400., 'string_test_3', date(2023, 3, 1), datetime(2023, 1, 3, 12, 0))\n",
        "])\n",
        "data_df = spark.createDataFrame(rdd, schema=['col_1', 'col_2', 'col_3', 'col_4', 'col_5'])\n",
        "```\n"
      ],
      "metadata": {
        "id": "roNwBK9WjUv-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Using Tuples\n",
        "\n",
        "We can create a tuple as a row and add each tuple as a separate row in the DataFrame.\n",
        "\n",
        "```\n",
        "import pandas as pd\n",
        "from datetime import datetime, date\n",
        "from pyspark.sql import Row\n",
        "rdd = spark.sparkContext.parallelize([\n",
        "    (100, 200., 'string_test_1', date(2023, 1, 1), datetime(2023, 1, 1, 12, 0)),\n",
        "    (200, 300., 'string_test_2', date(2023, 2, 1), datetime(2023, 1, 2, 12, 0)),\n",
        "    (300, 400., 'string_test_3', date(2023, 3, 1), datetime(2023, 1, 3, 12, 0))\n",
        "])\n",
        "data_df = spark.createDataFrame(rdd, schema=['col_1', 'col_2', 'col_3', 'col_4', 'col_5'])\n",
        "```"
      ],
      "metadata": {
        "id": "rOF2zjw_kGjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Viewing DataFrames\n",
        "\n",
        "data_df.show()"
      ],
      "metadata": {
        "id": "v8NcFCITeekk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Viewing top n rows\n",
        "\n",
        "data_df.show(2)"
      ],
      "metadata": {
        "id": "zMBKsimuk0xC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Viewing the schema of the dataframe\n",
        "\n",
        "data_df.printSchema()"
      ],
      "metadata": {
        "id": "DpFIX2b_lGk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## To view the data vertically\n",
        "data_df.show(1, vertical=True) # Shows the first record and its columns"
      ],
      "metadata": {
        "id": "wJQgQOsZlSH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.columns ## Shows the columns"
      ],
      "metadata": {
        "id": "wFT90AwWlePN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.select('col1', 'col2').describe().show() ## Shows the summary statistics"
      ],
      "metadata": {
        "id": "fgDII3dllidX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Collecting the Data\n",
        "\n",
        "A collect statement is used when we want to get all the data that is being processed in different clusters back to the driver. Make sure that the driver has enough memory to hold the processed data, to avoid out-of memory errors.\n",
        "\n",
        "\n",
        "```\n",
        "data_df.collect()\n",
        "```\n",
        "\n",
        "Use head, tail, take statements to avoid out-of-memory errors as they return only a subset of the data from the dataframe.\n",
        "\n",
        "\n",
        "```\n",
        "data_df.count()\n",
        "```\n",
        "\n",
        "Returns the number of rows present in the dataset.\n",
        "\n",
        "\n",
        "***tail(n) operation is expensive as it might have to scan the entire dataset first and then return the last n values***\n",
        "\n",
        "- **\"take\" and \"collect\" are used to retrieve data elements, with take being more suitable for small subsets and collect for retrieving all data**\n",
        "- **\"show\" is used for visual inspection, head retrieves the first rows as Row objects, and tail retrieves the last rows of the dataset**"
      ],
      "metadata": {
        "id": "l4oIKbhSl1LZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Converting a PySpark DataFrame to a Pandas DataFrame\n",
        "\n",
        "There are options to convert a PySpark DataFrame to a Pandas DataFrame. This option is ***toPandas().***\n",
        "\n",
        "Since Python inherently is not distributed, when a PySpark dataframe is converted to Pandas, the driver would need to collect all data in its memory and it should be able to contain all the memory to hold it, otherwise leading to out-of-memory error."
      ],
      "metadata": {
        "id": "S83lrkk4n-TB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How to manipulate data on rows and columns\n",
        "\n",
        "- 1. Selecting Columns\n",
        "\n",
        "Use column functions for manipulation at the columnar level in Spark Dataframe.\n",
        "```\n",
        "from pyspark.sql import Column\n",
        "data_df.select(data_df.col_3).show()\n",
        "### Or use this method\n",
        "\n",
        "data_df.select(data_df['col_3']).show()\n",
        "```\n",
        "- 2. Creating Columns\n",
        "\n",
        "We can use a withColumn() function to create a new column in a DataFrame. To create a new column, we would need to pass the column name and column values to fill the column with.\n",
        "\n",
        "\n",
        "```\n",
        "from pyspark.sql import functions as F\n",
        "data_df = data_df.withColumn(\"col_6\", F.lit(\"A\")) ## Here A is the literal\n",
        "data_df.show()\n",
        "```\n",
        "\n",
        "- 3. Dropping Columns\n",
        "\n",
        "Use drop() function to drop a column from Spark DataFrame.\n",
        "\n",
        "```\n",
        "data_df = data_df.drop(\"col_5\")\n",
        "data_df.show()\n",
        "```\n",
        "\n",
        "If we drop a non-existing column, it won't result in an error.\n",
        "\n",
        "- 4. Updating Columns\n",
        "\n",
        "```\n",
        "data_df.withColumn(\"col_2\", F.col(\"col_2\") / 100). show()\n",
        "```\n",
        "\n",
        "One thing to note here is the use of the col function when updating the column. This function is used for column-wise operators. If we donâ€™t use this function, our code will return an error.\n",
        "\n",
        "- 5. Renaming Columns\n",
        "```\n",
        "data_df = data_df.withColumnRenamed(\"col_3\", \"string_col\")\n",
        "data_df.show()\n",
        "```\n",
        "\n",
        "- 6. Find unique values in a column\n",
        "```\n",
        "data_df.select(\"col_6\").distinct().show()\n",
        "```\n",
        "\n",
        "To show the count of distinct values in a given column.\n",
        "```\n",
        "data_df.select(F.countDistinct(\"col_6\").alias(\"Total_Unique\")).show()\n",
        "```\n",
        "\n",
        "\n",
        "- 7. To change the case\n",
        "```\n",
        "from pyspark.sql.functions import upper\n",
        "data_df.withColumn('upper_string_col', upper(data_df.string_col)).show()\n",
        "```\n",
        "\n",
        "\n",
        "- 8. To filter out the records\n",
        "\n",
        "```\n",
        "data_df.filter(data_df.col_1 == 100).show()\n",
        "```\n",
        " - 9. Logical operators in a DataFrame\n",
        "```\n",
        "data_df.filter((data_df.col_1 == 100)\n",
        "                  & (data_df.col_6 == 'A')).show()\n",
        "```\n",
        "\n",
        "```\n",
        "data_df.filter((data_df.col_1 == 100)\n",
        "                  | (data_df.col_2 == 300.00)).show()\n",
        "```\n",
        "\n",
        "- 10. isin() operator\n",
        "\n",
        "The isin() function is used to find values in a DataFrame column that exist in a list.\n",
        "\n",
        "```\n",
        "list = [100, 200]\n",
        "data_df.filter(data_df.col_1.isin(list)).show()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "jptc0w07osNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DataType conversions\n",
        "\n",
        "Use cast function to change datatype\n",
        "\n",
        "```\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import StringType,BooleanType,DateType,IntegerType\n",
        "data_df_2 = data_df.withColumn(\"col_4\",col(\"col_4\").cast(StringType())) \\\n",
        "    .withColumn(\"col_1\",col(\"col_1\").cast(IntegerType()))\n",
        "data_df_2.printSchema()\n",
        "data_df.show()\n",
        "```\n",
        "\n",
        "```\n",
        "data_df_3 = data_df_2.selectExpr(\"cast(col_4 as date) col_4\",\n",
        "    \"cast(col_1 as long) col_1\")\n",
        "data_df_3.printSchema()\n",
        "data_df_3.show(truncate=False)\n",
        "```\n",
        "\n",
        "```\n",
        "data_df_3.createOrReplaceTempView(\"CastExample\")\n",
        "data_df_4 = spark.sql(\"SELECT DOUBLE(col_1), DATE(col_4) from CastExample\")\n",
        "data_df_4.printSchema()\n",
        "data_df_4.show(truncate=False)\n",
        "```\n"
      ],
      "metadata": {
        "id": "fjXqpowfuKso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dropping null values\n",
        "\n",
        ".dropna() function to drop null values\n",
        "\n",
        "### Dropping duplicates\n",
        "\n",
        ".dropDuplicates() to drop duplicate values"
      ],
      "metadata": {
        "id": "QKBp_rdIvSnH"
      }
    }
  ]
}