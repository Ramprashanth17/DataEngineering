{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00a2d18d-ca0e-40e9-b5b8-30c9f2a9ddb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, TimestampType\n",
    "\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"date_of_birth\", DateType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"member_since\", DateType(), True),\n",
    "    StructField(\"created_timestamp\", TimestampType(), True),\n",
    "    StructField(\"telephone\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d508a15-b2a6-4f03-a941-d4a933963bc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#### Read files using Datastream Reader API\n",
    "\n",
    "customer_df = spark.readStream.format(\"json\").schema(customer_schema).load(\"/Volumes/gizmobox/landing/operational_data/customers_stream/\") ##Can't infer schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4cc94d9-776f-4b7e-9e84-34a468e38232",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Transform the df to add two more cols filepath and ingestion date\n",
    "from pyspark.sql.functions import col, current_timestamp\n",
    "customer_transformed_df = ( \n",
    "                           customer_df.withColumn(\"filepath\", col(\"_metadata.file_path\"))\n",
    "                           .withColumn(\"ingestion_date\", current_timestamp())\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f47e52f-687c-4a8c-a91b-35edffa9c55d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Write the transformed data to Delta Table\n",
    "streaming_query = (\n",
    "    customer_transformed_df.writeStream.format(\"delta\").option(\"checkpointLocation\", \"/Volumes/gizmobox/landing/operational_data/customers_stream/_checkpoint_stream\" ).trigger(processingTime=\"2 minutes\").outputMode(\"append\").toTable(\"gizmobox.bronze.customers_stream\")\n",
    "     ##.option(\"mergeSchema\", \"true\") ## Merges the schema in case of new cols added without failing the stream on retry\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbbf516c-6a60-41cd-bd2f-99eb9430dc4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from gizmobox.bronze.customers_stream;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4619cd2-1537-4985-a534-399c7ea03932",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## to stop the streaming query\n",
    "streaming_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8672002f-bd8e-4c23-a911-2556db7d2888",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Processing modes in Spark Streaming\n",
    "1. Microbatch --> Spark collects the data in small intervals and process it as a batch, and after it captures new data. It's the default option\n",
    "2. Continuous Processing ---> Lower Fault tolerance, but process the data record by record\n",
    "\n",
    "##### Trigger:\n",
    "- Default is the no trigger which has 500 microseconds interval\n",
    "- fixed interval: .trigger(processingTime=\"2minutes\") User specified interval\n",
    "- Triggered Once: .trigger(once=True); Process all the data available as one micro-batch and stops (deprecated)\n",
    "- available now: .trigger(availableNow=True); Process all data available as multiple micro batch and stops (parallel processing)\n",
    "continuous\n",
    "\n",
    "##### outputMode:\n",
    "\n",
    "- Append(default): Writes the new rows arrived since the last micro-batch\n",
    "- complete: writes the entire result to sink every time\n",
    "- update: Writes only the rows that have changed since the last micro-batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30f2161e-fbbc-48c3-b9a8-af7009f969ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Checkpointing:\n",
    "\n",
    "It'a fault-tolerance mechanism that allows query to recover from failures and resume processing from where it left off without data loss or duplication\n",
    "\n",
    "Stores metadata about streaming query, execution plan\n",
    "\n",
    "Tracks processed offsets (readlogs (start)) and committed results (write logs (end)).\n",
    "\n",
    "Write-Ahead-Logs (WAL/offset logs) and checkpointing helps provide Fault Tolerance. \n",
    "\n",
    "Idempotent Sinks enables exactly once guarantees. (no duplication of data) if not idempotent sink, you need to take care of it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcefdc71-a513-4641-a322-459a92b75ff5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### AutoLoader\n",
    "\n",
    "Its a new structured streaming source designed for large scale efficient data ingestion. It incrementally and efficiently process new data files as they arrive.\n",
    "\n",
    "Traditional File Stream Source: Has inefficinet file listing, scalability issues, schema evolution problems\n",
    "- It constantly searches for any new incoming file this is okay for few files, but for millions of files it might cause memory problems\n",
    "- Deduplication is time consuming, if there are duplicate files it would take some time to correct it.\n",
    "- User has to give the schema and it can't be inferred, leading to data loss.\n",
    "\n",
    "Auto Loader: It uses efficient file detection using cloud services(cloud queue), in-memory replaced by rocks-db for scalability, schema evolution and resilency (rescued data, etc)\n",
    "\n",
    "```\n",
    "customer_df = spark.readStream\n",
    "                  .format(\"cloudFiles\") # enables autoloader\n",
    "                  .option(\"cloudFiles.format\", \"json\")\n",
    "                  .option(\"cloudFiles.useNotifications\", \"true\") ## to enable notifications using cloud svc provider\n",
    "                  .option(\"cloudFiles.schemaLocation\", \"Volumes/gizmobox/Landing/operational_data/customers_stream/_schema\")\n",
    "                  .option(\"cloudFiles.inferColumnTypes\",\"true\") ## to infer the schema\n",
    "                  .option(\"cloudFiles.schemaHints\", \"date_of_birth DATE ,member_since DATE\") ## to influence the behaviour of inferring the schema\n",
    "                  .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\") ## schema evolution\n",
    "                 \n",
    "                  .load(\"/Volumes/gizmobox/landing/operational_data/customers_autoLoader/\")\n",
    "\n",
    "```\n",
    "\n",
    "_rescued_date: string --> Special col added by autoloader to add any unhandled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d26d2d4-14c1-4f6c-9cec-b20bd0d0d4b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7408121933552656,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Structured_Streaming",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
