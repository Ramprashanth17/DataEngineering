{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHwh0TpGx6y2M9JP0vMe+s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ramprashanth17/DataEngineering/blob/main/Databricks_DE/AdvOps_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grouping Data and Types of Joins in Spark\n",
        "\n",
        "We can group data based on different cols in a DataFrame, and apply a different aggregations such as sum or avg to get a holistic view of data slices.\n",
        "\n",
        "```\n",
        "salary_data.groupby('Department')\n",
        "```\n",
        "\n",
        "This returns a grouped data object and can be assigned to a separate DF and more ops can be done on this data.\n",
        "\n",
        "Ex:\n",
        "```\n",
        "salary_data.groupby('Department').avg().show()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "egibBlJqmL9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A complex groupBy Statement\n",
        "\n",
        "groupBy can be used in complex data ops, such as multiple aggregations within a single groupBy statement.\n",
        "\n"
      ],
      "metadata": {
        "id": "Xz1oUqG2noAe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8y12SqBcmH4B"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, round\n",
        "salary_data.groupBy('Department')\\\n",
        "  .sum('Salary')\\\n",
        "  .withColumn('sum(Salary)', round(col('sum(Salary)'), 2)) \\\n",
        "  .withColumnRenamed('sum(Salary)', 'Salary') \\\n",
        "  .orderBy('Department')\\\n",
        "  .show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Joining DataFrames in Spark\n",
        "\n",
        "Join ops are used to combine data from two or more DataFrames based on a common column. It is essentially for merging datasets. aggregations, and relational operations.\n",
        "\n",
        "**.join()** is the method to join and its parameters are:\n",
        "\n",
        "- **other** The other dataframe to join with.\n",
        "- **on**: Cols on which to join the DataFrames\n",
        "- **how**: Type of join\n",
        "- **suffixes**: Suffixes to add to cols with the same name in both dataframes\n",
        "\n",
        "```\n",
        "DF1.join(DF2, on, how)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "cxnaTOuWo79P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "salary_data_with_id = [(1, \"John\", \"Field-eng\", 3500), \\\n",
        "    (2, \"Robert\", \"Sales\", 4000), \\\n",
        "    (3, \"Maria\", \"Finance\", 3500), \\\n",
        "    (4, \"Michael\", \"Sales\", 3000), \\\n",
        "    (5, \"Kelly\", \"Finance\", 3500), \\\n",
        "    (6, \"Kate\", \"Finance\", 3000), \\\n",
        "    (7, \"Martin\", \"Finance\", 3500), \\\n",
        "    (8, \"Kiran\", \"Sales\", 2200), \\\n",
        "  ]\n",
        "\n",
        "\n",
        "columns= [\"ID\", \"Employee\", \"Department\", \"Salary\"]\n",
        "salary_data_with_id = spark.createDataFrame(data = salary_data_with_id, schema = columns)\n",
        "salary_data_with_id.show()"
      ],
      "metadata": {
        "id": "T4U7RbiGqDMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "employee_data = [(1, \"NY\", \"M\"), \\\n",
        "    (2, \"NC\", \"M\"), \\\n",
        "    (3, \"NY\", \"F\"), \\\n",
        "    (4, \"TX\", \"M\"), \\\n",
        "    (5, \"NY\", \"F\"), \\\n",
        "    (6, \"AZ\", \"F\") \\\n",
        "  ]\n",
        "columns= [\"ID\", \"State\", \"Gender\"]\n",
        "employee_data = spark.createDataFrame(data = employee_data, schema = columns)\n",
        "employee_data.show()"
      ],
      "metadata": {
        "id": "Kmp_OdruqSoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **INNER JOIN**\n",
        "\n",
        "Used to join two DF's based on values that are common in both DF's. Any value that doesn't exist in any one of the DFs wouldn't be part of the resulting DF.\n",
        "\n",
        "***Default*** type of join in Spark.\n",
        "\n",
        "Use Case:\n",
        "\n",
        "Inner joins are useful for merging data when you are interested in common elements in both DataFrames – for example, joining sales data with customer data to see which customers made a purchase."
      ],
      "metadata": {
        "id": "7PNHxlQfrBgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "salary_data_with_id.join(employee_data,salary_data_with_id.ID == employee_data.ID, \"inner\").show()"
      ],
      "metadata": {
        "id": "dZmuPyTLqnm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **OUTER JOIN**\n",
        "\n",
        "AKA ***full outer join*** returns all rows from both DF's, filling missing values with ***null***.\n",
        "\n",
        "We should use an outer join when we want to join two DataFrames based on values that exist in both DataFrames, regardless of whether they don’t exist in the other DataFrame. Any values that exist in any one of the DataFrames would be part of the resulting DataFrame.\n",
        "\n",
        "**Use Case**:\n",
        "Use it when you want to include all records from both DataFrames while accommodating unmatched values- for ex, merging employee data with project data to see which employees are assigned to which projects, including those who are unassigned."
      ],
      "metadata": {
        "id": "G4wUZxLrr4a9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "salary_data_with_id.join(employee_data, salary_data_with_id.ID == employee_data.ID, \"outer\").show()"
      ],
      "metadata": {
        "id": "w9ThektPspxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **LEFT JOIN**\n",
        "\n",
        "Left Join returns all the rows from the left DF and the matched rows from the right DF. If there is no match in the right DF, the result will contain **null** values.\n",
        "\n",
        "**Use Case**\n",
        "\n",
        "Useful when you want to keep all the records from the left DF and only the matching records from the right DF. Ex: Merging customer data with transaction data to see which customers have made a purchase.\n",
        "\n"
      ],
      "metadata": {
        "id": "tdus3cF1tFRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "salary_data_with_id.join(employee_data, salary_data_with_id.ID == employee_data.ID, \"left\").show()"
      ],
      "metadata": {
        "id": "FgGKUM-bt0t0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **RIGHT JOIN**\n",
        "\n",
        "Similar to Left Join, but returns all the rows from the right DF and the matched rows from the left DF. Non matching rows from the left DF contain null values."
      ],
      "metadata": {
        "id": "B-llEgy1uIIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "salary_data_with_id.join(employee_data, salary_data_with_id.ID == employee_data.ID, \"right\").show()\n"
      ],
      "metadata": {
        "id": "xa0lD8Hhuasq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **CROSS JOIN**\n",
        "\n",
        "AKA Cartesian Join, combines each row from the left DF with every row from the right DF.\n",
        "\n",
        "Typically used when you want to explore all possible combinations of data, such as when generating test data.\n"
      ],
      "metadata": {
        "id": "miQPYsnoulWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **UNION**\n",
        "\n",
        "Union is used to join 2 DFs having similar schema."
      ],
      "metadata": {
        "id": "GpmOP_ejvFqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "salary_data_with_id_2 = [(1, \"John\", \"Field-eng\", 3500), \\\n",
        "    (2, \"Robert\", \"Sales\", 4000), \\\n",
        "    (3, \"Aliya\", \"Finance\", 3500), \\\n",
        "    (4, \"Nate\", \"Sales\", 3000), \\\n",
        "  ]\n",
        "columns2= [\"ID\", \"Employee\", \"Department\", \"Salary\"]\n",
        "salary_data_with_id_2 = spark.createDataFrame(data = salary_data_with_id_2, schema = columns2)\n",
        "salary_data_with_id_2.printSchema()\n",
        "salary_data_with_id_2.show(truncate=False)"
      ],
      "metadata": {
        "id": "-8gAPYu5vabm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unionDF = salary_data_with_id.union(salary_data_with_id_2)\n",
        "unionDF.show(truncate=False)"
      ],
      "metadata": {
        "id": "BVr_p5CfvbQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reading and writing CSV files in Spark\n",
        "\n",
        "```\n",
        "salary_data_with_id.write.csv('salary_data.csv', mode='overwrite', header=True)\n",
        "spark.read.csv('/salary_data.csv', header=True).show()\n",
        "```\n",
        "```\n",
        "from pyspark.sql.types import *\n",
        "filePath = '/salary_data.csv'\n",
        "columns= [\"ID\", \"State\", \"Gender\"]\n",
        "schema = StructType([\n",
        "      StructField(\"ID\", IntegerType(),True),\n",
        "  StructField(\"State\",  StringType(),True),\n",
        "  StructField(\"Gender\",  StringType(),True)\n",
        "])\n",
        "read_data = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schema).load(filePath)\n",
        "read_data.show()\n",
        "```"
      ],
      "metadata": {
        "id": "4SDRZsVH8klN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reading and writing in Parquet files\n",
        "\n",
        "```\n",
        "salary_data_with_id.write.parquet('salary_data.parquet', mode='overwrite')\n",
        "spark.read.parquet(' /salary_data.parquet').show()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "npg0IK3r9nvK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reading and writing Delta Files\n",
        "\n",
        "The Delta file format is an open format that is more optimized than Parquet and other columnar formats. When the data is stored in Delta format, you will notice that the underlying files are in Parquet. The Delta format adds a transactional log on top of Parquet files to make data reads and writes a lot more efficient.\n",
        "\n",
        "```\n",
        "salary_data_with_id.write.format(\"delta\").save(\"/FileStore/tables/salary_data_with_id\", mode='overwrite')\n",
        "df = spark.read.load(\"/FileStore/tables/salary_data_with_id\")\n",
        "df.show()\n",
        "```"
      ],
      "metadata": {
        "id": "xLqursc7-UlY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### UDFs in Apache Spark\n",
        "\n",
        "UDFs are custom functions that are created by users to perform specific ops on data within Spark. They extend the range of transformations and ops you can apply to your data.\n",
        "\n",
        "To use UDFs, you need to create/ define function and register them with Spark. You can define UDFs for both SQL and Dataframe operations.\n",
        "\n",
        "```\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "# Define a UDF in Python\n",
        "def my_udf_function(input_param):\n",
        "# Your custom logic here\n",
        "return processed_value\n",
        "# Register the UDF with Spark\n",
        "my_udf = udf(my_udf_function, IntegerType())\n",
        "# Using the UDF in a DataFrame operation\n",
        "df = df.withColumn(\"new_column\", my_udf(df[\"input_column\"]))\n",
        "```"
      ],
      "metadata": {
        "id": "X3bYFLlE-9vc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QLF4cNOv9YNL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}